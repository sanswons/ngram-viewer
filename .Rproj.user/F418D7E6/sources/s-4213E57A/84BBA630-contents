library(tm)
library(ngram)
library(ggplot2)
library(NLP)
library(openNLP)
library(wordnet)
library(qdap)
library(hunspell)
library(ggthemes)

Sys.setenv(WNHOME = "WordNet/2.1")
setDict("WordNet/2.1/dict")

extract_texts = function(path){
  filenames = list.files(path)
  
  # Create a dataframe to store texts for an all years
  texts_df = data.frame(year = character(), text = character() , stringsAsFactors = FALSE)
  
  # Go through all folders
  for (i in 1:length(filenames)) 
  {
    # Check if it is an year
    if (is.na(as.numeric(filenames[i]))==FALSE)
    {
      year = filenames[i]
      path = paste(c('task/', year,  '/'), collapse = '')
      textnames = list.files(path)
      
      for (textname in textnames) 
      {
        fulltext = ''
        
        # Check if it is a text file
        if (endsWith(textname,'.txt'))
        {
          path = paste(c('task/', year, '/', textname),collapse = '')
          
          # Clean the text 
          text = cleaning(scan(path, what = 'character'))
          
          # combine texts seperated by colon
          fulltext = paste(c(fulltext, text), collapse = ';')
        }
      }
      newRow = data.frame(year, fulltext, stringsAsFactors = FALSE)
      texts_df = rbind(texts_df, newRow)
    }
  }
  print('Created df')
  return(texts_df)
  
}
      

cleaning = function(text){
  text = concatenate(text, collapse = ' ')
  
  # Uniform case
  text = tolower(text)
  
  #Remove urls
  text = gsub('http\\S+\\s*', ',', text)
  
  #Remove punctuation
  text = gsub("[[:punct:]]", "", text)
  
  #Remove extra spaces
  text = gsub('\\s+',' ', text)
  return (text)
}

create_ngrams_allbooks = function(df, max_ngrams){
  nmax = max_ngrams
  
  for (i in 1:length(df$year))
  {
    for (j in 1:nmax)
    {
      # finds all n-grams of length n
      output = ngram(str = df[i,2], n = j)
      
      # Creates a table with counts of n-grams
      table = get.phrasetable(output)
      
      # Saves as a csv in a folder called ngrams
      filename = paste(c('ngrams/',df[i,1],'_',j),collapse = '')
      write.csv(as.data.frame(table), file = filename)
    }
  }
  print('Created csv files')
}

preprocess_phrase = function(phrase){
  #Pre-process phrase to lower case and trim spaces
  phrase = tolower(paste(c(trimws(phrase), ' '),collapse = ''))
  
  #Remove punctuation
  phrase = gsub("[[:punct:]]", "", phrase)
  
  #Remove extra spaces
  phrase = gsub('\\s+',' ', phrase)
  
  return(phrase)
}

search_ngrams = function(phrase, nyears, filenames){
  print('Searching')
  
  #Pre-process phrase to lower case and trim spaces
  phrase = preprocess_phrase(phrase)
  
  # find the n-gram length
  num_words = as.character(length(strsplit(phrase,split = " ")[[1]]))
  
  # Initialize a dataframe to store all frequencies
  # This dataframe contains the phrase, year, frequency
  freqs_allbooks = data.frame()
  
  for (i in 1:length(nyears))
  {
    # Check for the required csv
    path = paste(c('ngrams/',nyears[i],'_',num_words), collapse = '')
    freqs = read.csv(path)
    freqs$ngrams = as.character(freqs$ngrams)
    
    # Find the index of the n-gram 
    index = match(phrase,freqs$ngrams,nomatch = 0)
    if (index==0)
    {
      frequency = 0 
    }
    else
    {
      frequency = freqs$freq[index]  
    }
    newRow = data.frame(year = nyears[i], freq = frequency, phrase = phrase, stringsAsFactors = FALSE)
    freqs_allbooks = rbind(freqs_allbooks,newRow)
  }
  return(freqs_allbooks)
}

create_plots = function(query){
  filenames = list.files('ngrams/')
  
  # Split the search query
  phrases = strsplit(query,split = ",")
  print('Started')
  freqs = data.frame()
  for (i in 1:length(phrases[[1]])){
    #check spelling
    phrase = check_spelling(phrases[[1]][i])
    #print(phrase)
    
    # Create frequency table for a phrase
    newfreqs = search_ngrams(phrase, texts_df$year, filenames)

    # Bind the frequency tables of all phrases
    freqs = rbind(freqs, newfreqs)
  }
  
  # Plot the frequencies
  p = ggplot(data = freqs, aes( x = year, y = freq, colour = phrase, group = phrase)) + geom_point(size = 4) + geom_line(size = 1.25) +
    theme_few()
  
  return (p)
}

check_spelling = function(phrase){
  bad_words = hunspell_find(phrase)[[1]]
  phrase = strsplit(phrase, split = ' ')[[1]]
  
  if (length(bad_words)!=0){
    for (i in 1:length(bad_words)) {
      index = match(bad_words[i], phrase)
      phrase[index] = hunspell_suggest(bad_words[[i]])[[1]][1]
    }
  }
  return(paste(phrase,collapse = ' '))
}

find_synonyms = function(query){
  
  # check if string is empty
  if(Trim(query)!=''){
    print('Yesssssssss')
    w = extract_last_word(Trim(query))
    
    #correct spelling
    w = check_spelling(w)
    syns = list()
    
    #grouping tags
    noun_tags = c('NN', 'NNP', 'NNS', 'NNPS')
    verb_tags = c( 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ')
    adj_tags = c('JJ', 'JJR', 'JJS')
    adv_tags = c('RB','RBR', 'RBS')
    
    tag = tagPOS(w)
    
    #find synonyms using wornet and qdap
    if (tag %in% noun_tags){
      syns = c(syns,wordnet::synonyms(w,'NOUN'))
    }
    else if (tag %in% verb_tags){
      syns = c(syns,wordnet::synonyms(w,'VERB'))
    }
    else if (tag %in% adj_tags){
      syns = c(syns,qdap::synonyms(w, return.list = FALSE))
      syns = c(syns,wordnet::synonyms(w,'ADJ'))
    }
    else if(tag %in% adv_tags){
      syns = c(syns,qdap::synonyms(w, return.list = FALSE))
    }
  
    suggestions  = create_synonyms_phrase(query, unlist(syns), w)
  }
  
  else{
    suggestions[['Not found']] = 'Not found'
  }
  print(suggestions)
  return(suggestions)
}

extract_last_word=function(query){
  phrases = strsplit(query, split = ',')[[1]]
  last_phrase = tail(phrases, 1)
  last_word = tail(strsplit(last_phrase,' ')[[1]],1)
  return(last_word)
}

extract_last_phrase = function(query){
  phrases = strsplit(query, split = ',')[[1]]
  last_phrase = tail(phrases, 1)
  return (last_phrase)
}

#use the synonyms of the last word and replace it in the n-gram
create_synonyms_phrase= function(query, syns, word){
  last_phrase = check_spelling(extract_last_phrase(query))
  suggestions = list()
  for(syn in syns){
    suggestions[[gsub(word, syn, last_phrase)]] = gsub(word, syn, last_phrase)
  }
  if (length(suggestions)==0){
    suggestions[['Not found']] = 'Not found'
  }
  return(suggestions)
}


tagPOS <-  function(x, ...) {
  s <- as.String(x)
  word_token_annotator <- Maxent_Word_Token_Annotator()
  a2 <- Annotation(1L, "sentence", 1L, nchar(s))
  a2 <- NLP::annotate(s, word_token_annotator, a2)
  a3 <- NLP::annotate(s, Maxent_POS_Tag_Annotator(), a2)
  a3w <- a3[a3$type == "word"]
  POStags <- unlist(lapply(a3w$features, `[[`, "POS"))
  return (POStags)
}


texts_df = extract_texts('task/')
create_ngrams_allbooks(texts_df, max_ngrams = 5)



